{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|seqno|Name|\n",
      "+-----+----+\n",
      "|    1|  VJ|\n",
      "|    2| bnm|\n",
      "|    3| mnb|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "columns = ['seqno', 'Name']\n",
    "data = [(1, \"VJ\"), (2, \"bnm\"), (3, \"mnb\")]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|seqno|Changed Name|\n",
      "+-----+------------+\n",
      "|    1|         VJ |\n",
      "|    2|        Bnm |\n",
      "|    3|        Mnb |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def to_upper_case(str):\n",
    "    rel=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "        rel += x[0:1].upper()+x[1:]+\" \"\n",
    "    return rel\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "convert = udf(lambda z:to_upper_case(z))\n",
    "\n",
    "df.select(col('seqno'), convert(col('Name')).alias('Changed Name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------+\n",
      "|seqno|Name|changed_name|\n",
      "+-----+----+------------+\n",
      "|    1|  VJ|         VJ |\n",
      "|    2| bnm|        Bnm |\n",
      "|    3| mnb|        Mnb |\n",
      "+-----+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"changed_name\", convert(col('Name'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|seqno|Names|\n",
      "+-----+-----+\n",
      "|    1|  VJ |\n",
      "|    2| Bnm |\n",
      "|    3| Mnb |\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"changed_to_upper\", to_upper_case)\n",
    "df.createOrReplaceTempView(\"new_name\")\n",
    "spark.sql('select seqno, changed_to_upper(Name) as Names from new_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+----------------+\n",
      "| id|color|value|value_minus_mean|\n",
      "+---+-----+-----+----------------+\n",
      "|  2| blue|   20|           -10.0|\n",
      "|  4| blue|   40|            10.0|\n",
      "|  1|  red|   10|           -10.0|\n",
      "|  3|  red|   30|            10.0|\n",
      "+---+-----+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Create SparkSession with explicit configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ApplyInPandasExample\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "    .config(\"spark.pyspark.python\", sys.executable) \\\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# Create a simple DataFrame\n",
    "data = [(1, \"red\", 10), (2, \"blue\", 20), (3, \"red\", 30), (4, \"blue\", 40)]\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"value\", IntegerType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Define a simple function to apply\n",
    "def subtract_mean(pdf):\n",
    "    pdf['value_minus_mean'] = pdf['value'] - pdf['value'].mean()\n",
    "    return pdf\n",
    "\n",
    "# Define complete output schema\n",
    "output_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"value\", IntegerType(), True),\n",
    "    StructField(\"value_minus_mean\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Apply the function using applyInPandas\n",
    "result = df.groupBy(\"color\").applyInPandas(subtract_mean, schema=output_schema)\n",
    "\n",
    "# Show the result\n",
    "result.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  1| 10|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|carrot|  3| 30|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|black|carrot|  6| 60|\n",
      "|  red|banana|  7| 70|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ApplyInPandasExample\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "    .config(\"spark.pyspark.python\", sys.executable) \\\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable) \\\n",
    "    .getOrCreate()\n",
    "df = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|color|avg(v1)|avg(v2)|\n",
      "+-----+-------+-------+\n",
      "|  red|    4.8|   48.0|\n",
      "| blue|    3.0|   30.0|\n",
      "|black|    6.0|   60.0|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('color').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|black|carrot|  0| 60|\n",
      "| blue|banana| -1| 20|\n",
      "| blue| grape|  1| 40|\n",
      "|  red|banana| -3| 10|\n",
      "|  red|carrot| -1| 30|\n",
      "|  red|carrot|  0| 50|\n",
      "|  red|banana|  2| 70|\n",
      "|  red| grape|  3| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def plus_mean(pandas_df):\n",
    "    return pandas_df.assign(v1 = pandas_df.v1-pandas_df.v1.mean())\n",
    "\n",
    "df.groupBy('color').applyInPandas(plus_mean, schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized DataFrame:\n",
      "+---+--------------------+\n",
      "|  a|                   b|\n",
      "+---+--------------------+\n",
      "|  1| -0.9462724090245991|\n",
      "|  2| -0.7209694544949327|\n",
      "|  3|-0.27036354543559976|\n",
      "|  4| 0.40554531815339956|\n",
      "|  5|  1.5320600908017319|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NormalizationExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "df1 = spark.createDataFrame([\n",
    "    Row(a=1, b=2.0),\n",
    "    Row(a=2, b=3.0),\n",
    "    Row(a=3, b=5.0),\n",
    "    Row(a=4, b=8.0),\n",
    "    Row(a=5, b=13.0)\n",
    "])\n",
    "\n",
    "# Define a schema for the output DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"a\", LongType(), True),\n",
    "    StructField(\"b\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Define a pandas UDF for row-wise normalization\n",
    "def normalize(pdf):\n",
    "    mean = pdf['b'].mean()\n",
    "    std = pdf['b'].std()\n",
    "    if std != 0:\n",
    "        pdf['b'] = (pdf['b'] - mean) / std\n",
    "    else:\n",
    "        pdf['b'] = 0  # Handle the case where std is zero\n",
    "    return pdf\n",
    "\n",
    "# Apply applyInPandas on DataFrame\n",
    "df_normalized = df1.groupby().applyInPandas(normalize, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(\"Normalized DataFrame:\")\n",
    "df_normalized.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|celcius|\n",
      "+---+-------+\n",
      "|  1|   25.0|\n",
      "|  2|   30.0|\n",
      "|  3|   35.0|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "spark = SparkSession.builder.appName('celcius to farhenhite').getOrCreate()\n",
    "c_f_df = spark.createDataFrame([(1,25.0),(2,30.0),(3,35.0)],schema='id bigint, celcius double')\n",
    "c_f_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+\n",
      "| id|celcius|Farhenite|\n",
      "+---+-------+---------+\n",
      "|  1|     25|       77|\n",
      "|  2|     30|       86|\n",
      "|  3|     35|       95|\n",
      "+---+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "schema = StructType([\n",
    "                     StructField('id', LongType(), True), \n",
    "                     StructField('celcius', LongType(), True), \n",
    "                     StructField('Farhenite', LongType(), True)\n",
    "                    ])\n",
    "\n",
    "def cel_far(series):\n",
    "    # for val in series:\n",
    "    #     series[F]\n",
    "    series['Farhenite'] = series['celcius'] * (9/5) + 32\n",
    "    return series\n",
    "\n",
    "c_f_df.groupBy().applyInPandas(cel_far, schema=schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+---+---+\n",
      "|        time| id| v1| v2|\n",
      "+------------+---+---+---+\n",
      "|202407130921|  1|1.0|  x|\n",
      "|202407130922|  1|3.0|  x|\n",
      "|202407130921|  2|2.0|  y|\n",
      "|202407130922|  2|4.0|  y|\n",
      "+------------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(202407130921, 1, 1.0),\n",
    "                             (202407130921, 2, 2.0),\n",
    "                             (202407130922, 1, 3.0),\n",
    "                             (202407130922, 2, 4.0)],('time', 'id', 'v1'))\n",
    "df2 = spark.createDataFrame([(202407130921, 1, 'x'),\n",
    "                             (202407130921, 2, 'y'),(202407130922, 1, 'x'),(202407130922, 2, 'y')],('time', 'id', 'v2'))\n",
    "def merge_ordered(l, r):\n",
    "    return pd.merge_ordered(l, r)\n",
    "\n",
    "df1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(merge_ordered, schema='time long, id int, v1 double, v2 string').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|seqno|Name|\n",
      "+-----+----+\n",
      "|    1|  VJ|\n",
      "|    2| bnm|\n",
      "|    3| mnb|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "columns = ['seqno', 'Name']\n",
    "data = [(1, \"VJ\"), (2, \"bnm\"), (3, \"mnb\")]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|seqno|Changed Name|\n",
      "+-----+------------+\n",
      "|    1|         VJ |\n",
      "|    2|        Bnm |\n",
      "|    3|        Mnb |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def to_upper_case(str):\n",
    "    rel=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "        rel += x[0:1].upper()+x[1:]+\" \"\n",
    "    return rel\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "convert = udf(lambda z:to_upper_case(z))\n",
    "\n",
    "df.select(col('seqno'), convert(col('Name')).alias('Changed Name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------+\n",
      "|seqno|Name|changed_name|\n",
      "+-----+----+------------+\n",
      "|    1|  VJ|         VJ |\n",
      "|    2| bnm|        Bnm |\n",
      "|    3| mnb|        Mnb |\n",
      "+-----+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"changed_name\", convert(col('Name'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|seqno|Names|\n",
      "+-----+-----+\n",
      "|    1|  VJ |\n",
      "|    2| Bnm |\n",
      "|    3| Mnb |\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"changed_to_upper\", to_upper_case)\n",
    "df.createOrReplaceTempView(\"new_name\")\n",
    "spark.sql('select seqno, changed_to_upper(Name) as Names from new_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+----------------+\n",
      "| id|color|value|value_minus_mean|\n",
      "+---+-----+-----+----------------+\n",
      "|  2| blue|   20|           -10.0|\n",
      "|  4| blue|   40|            10.0|\n",
      "|  1|  red|   10|           -10.0|\n",
      "|  3|  red|   30|            10.0|\n",
      "+---+-----+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Create SparkSession with explicit configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ApplyInPandasExample\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "    .config(\"spark.pyspark.python\", sys.executable) \\\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# Create a simple DataFrame\n",
    "data = [(1, \"red\", 10), (2, \"blue\", 20), (3, \"red\", 30), (4, \"blue\", 40)]\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"value\", IntegerType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Define a simple function to apply\n",
    "def subtract_mean(pdf):\n",
    "    pdf['value_minus_mean'] = pdf['value'] - pdf['value'].mean()\n",
    "    return pdf\n",
    "\n",
    "# Define complete output schema\n",
    "output_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"value\", IntegerType(), True),\n",
    "    StructField(\"value_minus_mean\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Apply the function using applyInPandas\n",
    "result = df.groupBy(\"color\").applyInPandas(subtract_mean, schema=output_schema)\n",
    "\n",
    "# Show the result\n",
    "result.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  1| 10|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|carrot|  3| 30|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|black|carrot|  6| 60|\n",
      "|  red|banana|  7| 70|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ApplyInPandasExample\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "    .config(\"spark.pyspark.python\", sys.executable) \\\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable) \\\n",
    "    .getOrCreate()\n",
    "df = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|color|avg(v1)|avg(v2)|\n",
      "+-----+-------+-------+\n",
      "|  red|    4.8|   48.0|\n",
      "| blue|    3.0|   30.0|\n",
      "|black|    6.0|   60.0|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('color').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|black|carrot|  0| 60|\n",
      "| blue|banana| -1| 20|\n",
      "| blue| grape|  1| 40|\n",
      "|  red|banana| -3| 10|\n",
      "|  red|carrot| -1| 30|\n",
      "|  red|carrot|  0| 50|\n",
      "|  red|banana|  2| 70|\n",
      "|  red| grape|  3| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def plus_mean(pandas_df):\n",
    "    return pandas_df.assign(v1 = pandas_df.v1-pandas_df.v1.mean())\n",
    "\n",
    "df.groupBy('color').applyInPandas(plus_mean, schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized DataFrame:\n",
      "+---+--------------------+\n",
      "|  a|                   b|\n",
      "+---+--------------------+\n",
      "|  1| -0.9462724090245991|\n",
      "|  2| -0.7209694544949327|\n",
      "|  3|-0.27036354543559976|\n",
      "|  4| 0.40554531815339956|\n",
      "|  5|  1.5320600908017319|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NormalizationExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "df1 = spark.createDataFrame([\n",
    "    Row(a=1, b=2.0),\n",
    "    Row(a=2, b=3.0),\n",
    "    Row(a=3, b=5.0),\n",
    "    Row(a=4, b=8.0),\n",
    "    Row(a=5, b=13.0)\n",
    "])\n",
    "\n",
    "# Define a schema for the output DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"a\", LongType(), True),\n",
    "    StructField(\"b\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Define a pandas UDF for row-wise normalization\n",
    "def normalize(pdf):\n",
    "    mean = pdf['b'].mean()\n",
    "    std = pdf['b'].std()\n",
    "    if std != 0:\n",
    "        pdf['b'] = (pdf['b'] - mean) / std\n",
    "    else:\n",
    "        pdf['b'] = 0  # Handle the case where std is zero\n",
    "    return pdf\n",
    "\n",
    "# Apply applyInPandas on DataFrame\n",
    "df_normalized = df1.groupby().applyInPandas(normalize, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(\"Normalized DataFrame:\")\n",
    "df_normalized.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|celcius|\n",
      "+---+-------+\n",
      "|  1|   25.0|\n",
      "|  2|   30.0|\n",
      "|  3|   35.0|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "spark = SparkSession.builder.appName('celcius to farhenhite').getOrCreate()\n",
    "c_f_df = spark.createDataFrame([(1,25.0),(2,30.0),(3,35.0)],schema='id bigint, celcius double')\n",
    "c_f_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+\n",
      "| id|celcius|Farhenite|\n",
      "+---+-------+---------+\n",
      "|  1|     25|       77|\n",
      "|  2|     30|       86|\n",
      "|  3|     35|       95|\n",
      "+---+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "schema = StructType([\n",
    "                     StructField('id', LongType(), True), \n",
    "                     StructField('celcius', LongType(), True), \n",
    "                     StructField('Farhenite', LongType(), True)\n",
    "                    ])\n",
    "\n",
    "def cel_far(series):\n",
    "    # for val in series:\n",
    "    #     series[F]\n",
    "    series['Farhenite'] = series['celcius'] * (9/5) + 32\n",
    "    return series\n",
    "\n",
    "c_f_df.groupBy().applyInPandas(cel_far, schema=schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+---+---+\n",
      "|        time| id| v1| v2|\n",
      "+------------+---+---+---+\n",
      "|202407130921|  1|1.0|  x|\n",
      "|202407130922|  1|3.0|  x|\n",
      "|202407130921|  2|2.0|  y|\n",
      "|202407130922|  2|4.0|  y|\n",
      "+------------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(202407130921, 1, 1.0),\n",
    "                             (202407130921, 2, 2.0),\n",
    "                             (202407130922, 1, 3.0),\n",
    "                             (202407130922, 2, 4.0)],('time', 'id', 'v1'))\n",
    "df2 = spark.createDataFrame([(202407130921, 1, 'x'),\n",
    "                             (202407130921, 2, 'y'),(202407130922, 1, 'x'),(202407130922, 2, 'y')],('time', 'id', 'v2'))\n",
    "def merge_ordered(l, r):\n",
    "    return pd.merge_ordered(l, r)\n",
    "\n",
    "new_df = df1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(merge_ordered, schema='time long, id int, v1 double, v2 string')\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
