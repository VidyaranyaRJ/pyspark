{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|seqno|Name|\n",
      "+-----+----+\n",
      "|    1|  VJ|\n",
      "|    2| bnm|\n",
      "|    3| mnb|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "columns = ['seqno', 'Name']\n",
    "data = [(1, \"VJ\"), (2, \"bnm\"), (3, \"mnb\")]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|seqno|Changed Name|\n",
      "+-----+------------+\n",
      "|    1|         VJ |\n",
      "|    2|        Bnm |\n",
      "|    3|        Mnb |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def to_upper_case(str):\n",
    "    rel=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "        rel += x[0:1].upper()+x[1:]+\" \"\n",
    "    return rel\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "convert = udf(lambda z:to_upper_case(z))\n",
    "\n",
    "df.select(col('seqno'), convert(col('Name')).alias('Changed Name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------+\n",
      "|seqno|Name|changed_name|\n",
      "+-----+----+------------+\n",
      "|    1|  VJ|         VJ |\n",
      "|    2| bnm|        Bnm |\n",
      "|    3| mnb|        Mnb |\n",
      "+-----+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"changed_name\", convert(col('Name'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|seqno|Names|\n",
      "+-----+-----+\n",
      "|    1|  VJ |\n",
      "|    2| Bnm |\n",
      "|    3| Mnb |\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"changed_to_upper\", to_upper_case)\n",
    "df.createOrReplaceTempView(\"new_name\")\n",
    "spark.sql('select seqno, changed_to_upper(Name) as Names from new_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+----------------+\n",
      "| id|color|value|value_minus_mean|\n",
      "+---+-----+-----+----------------+\n",
      "|  2| blue|   20|           -10.0|\n",
      "|  4| blue|   40|            10.0|\n",
      "|  1|  red|   10|           -10.0|\n",
      "|  3|  red|   30|            10.0|\n",
      "+---+-----+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Create SparkSession with explicit configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ApplyInPandasExample\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "    .config(\"spark.pyspark.python\", sys.executable) \\\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# Create a simple DataFrame\n",
    "data = [(1, \"red\", 10), (2, \"blue\", 20), (3, \"red\", 30), (4, \"blue\", 40)]\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"value\", IntegerType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Define a simple function to apply\n",
    "def subtract_mean(pdf):\n",
    "    pdf['value_minus_mean'] = pdf['value'] - pdf['value'].mean()\n",
    "    return pdf\n",
    "\n",
    "# Define complete output schema\n",
    "output_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"value\", IntegerType(), True),\n",
    "    StructField(\"value_minus_mean\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Apply the function using applyInPandas\n",
    "result = df.groupBy(\"color\").applyInPandas(subtract_mean, schema=output_schema)\n",
    "\n",
    "# Show the result\n",
    "result.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  1| 10|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|carrot|  3| 30|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|black|carrot|  6| 60|\n",
      "|  red|banana|  7| 70|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ApplyInPandasExample\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "    .config(\"spark.pyspark.python\", sys.executable) \\\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable) \\\n",
    "    .getOrCreate()\n",
    "df = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|color|avg(v1)|avg(v2)|\n",
      "+-----+-------+-------+\n",
      "|  red|    4.8|   48.0|\n",
      "| blue|    3.0|   30.0|\n",
      "|black|    6.0|   60.0|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('color').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|black|carrot|  0| 60|\n",
      "| blue|banana| -1| 20|\n",
      "| blue| grape|  1| 40|\n",
      "|  red|banana| -3| 10|\n",
      "|  red|carrot| -1| 30|\n",
      "|  red|carrot|  0| 50|\n",
      "|  red|banana|  2| 70|\n",
      "|  red| grape|  3| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def plus_mean(pandas_df):\n",
    "    return pandas_df.assign(v1 = pandas_df.v1-pandas_df.v1.mean())\n",
    "\n",
    "df.groupBy('color').applyInPandas(plus_mean, schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|  a|   b|\n",
      "+---+----+\n",
      "|  1|NULL|\n",
      "|  2|NULL|\n",
      "|  3|NULL|\n",
      "|  4|NULL|\n",
      "|  5|NULL|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# import sys\n",
    "# import os\n",
    "# import pyarrow\n",
    "# import pandas as pd\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "\n",
    "# # Set environment variables\n",
    "# os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# # Create SparkSession with explicit configurations\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"ApplyInPandasExample\") \\\n",
    "#     .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "#     .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "#     .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "#     .config(\"spark.pyspark.python\", sys.executable) \\\n",
    "#     .config(\"spark.pyspark.driver.python\", sys.executable) \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "df1 = spark.createDataFrame([\n",
    "    Row(a=1, b=2.0),\n",
    "    Row(a=2, b=3.0),\n",
    "    Row(a=3, b=5.0),\n",
    "    Row(a=4, b=8.0),\n",
    "    Row(a=5, b=13.0)\n",
    "])\n",
    "\n",
    "# Define a pandas UDF for row-wise normalization\n",
    "#@pandas_udf(\"a long, b double\")\n",
    "def normalize(iterator):\n",
    "    for pdf in iterator:\n",
    "        yield pdf.assign(b=(pdf.b - pdf.b.mean()) / pdf.b.std())\n",
    "\n",
    "# Apply mapInPandas on DataFrame\n",
    "df1.mapInPandas(normalize, schema=df1.schema).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
