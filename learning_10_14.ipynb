{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.enabled', True)\n",
    "df = spark.createDataFrame([['a', 13, \"ucd\"],['b', 15, \"ucb\"], ['c', 19,\"uccs\"]],schema=['name', 'age', 'university'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()\n",
    "\n",
    "df = df.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "|name|age|university|\n",
      "+----+---+----------+\n",
      "|   b| 15|       ucb|\n",
      "|   c| 19|      uccs|\n",
      "|   a| 13|       ucd|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|name|avg(age)|\n",
      "+----+--------+\n",
      "|   b|    15.0|\n",
      "|   c|    19.0|\n",
      "|   a|    13.0|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('name').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|   b|\n",
      "|   c|\n",
      "|   a|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+----------+\n",
      "|name|age|university|upper_name|\n",
      "+----+---+----------+----------+\n",
      "|   b| 15|       ucb|         B|\n",
      "|   c| 19|      uccs|         C|\n",
      "|   a| 13|       ucd|         A|\n",
      "+----+---+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "df.withColumn('upper_name', upper(df.name)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text('teenagers.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|           value|\n",
      "+----------------+\n",
      "|   a,13,abnjvjdv|\n",
      "|     b,14,jvvhsf|\n",
      "|     c,1,jdvjkhv|\n",
      "|    d,15,jvhggir|\n",
      "|    e,24,hhfrwnf|\n",
      "|  f,18,iueairier|\n",
      "|g,10,iwruiwejrsf|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.value.contains('j')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"test\").setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrjav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a,13,abnjvjdv',\n",
       " 'b,14,jvvhsf',\n",
       " 'c,1,jdvjkhv',\n",
       " 'd,15,jvhggir',\n",
       " 'e,24,hhfrwnf',\n",
       " 'f,18,iueairier',\n",
       " 'g,10,iwruiwejrsf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sql_sc = SQLContext(sc)\n",
    "df1 = sc.textFile('teenagers.txt')\n",
    "df1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd = df1.map(lambda x:(x,)).toDF(['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|           value|\n",
      "+----------------+\n",
      "|   a,13,abnjvjdv|\n",
      "|     b,14,jvvhsf|\n",
      "|     c,1,jdvjkhv|\n",
      "|    d,15,jvhggir|\n",
      "|  f,18,iueairier|\n",
      "|g,10,iwruiwejrsf|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rdd.filter(df_rdd.value.contains('1')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = spark.sparkContext.parallelize(range(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|tripletID|   triplet|\n",
      "+---------+----------+\n",
      "|  0, 1, 2| [0, 1, 2]|\n",
      "|  0, 1, 3| [0, 1, 3]|\n",
      "|  0, 1, 4| [0, 1, 4]|\n",
      "|  0, 1, 5| [0, 1, 5]|\n",
      "|  0, 1, 6| [0, 1, 6]|\n",
      "|  0, 1, 7| [0, 1, 7]|\n",
      "|  0, 1, 8| [0, 1, 8]|\n",
      "|  0, 1, 9| [0, 1, 9]|\n",
      "| 0, 1, 10|[0, 1, 10]|\n",
      "| 0, 1, 11|[0, 1, 11]|\n",
      "| 0, 1, 12|[0, 1, 12]|\n",
      "| 0, 1, 13|[0, 1, 13]|\n",
      "| 0, 1, 14|[0, 1, 14]|\n",
      "| 0, 1, 15|[0, 1, 15]|\n",
      "| 0, 1, 16|[0, 1, 16]|\n",
      "| 0, 1, 17|[0, 1, 17]|\n",
      "| 0, 1, 18|[0, 1, 18]|\n",
      "| 0, 1, 19|[0, 1, 19]|\n",
      "| 0, 1, 20|[0, 1, 20]|\n",
      "| 0, 1, 21|[0, 1, 21]|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "combinationsDF = list(combinations(val.collect(), 3))\n",
    "\n",
    "data1 = [(f\"{sorted(row)[0]}, {sorted(row)[1]}, {sorted(row)[2]}\", list(sorted(row))) for row in combinationsDF]\n",
    "df2 = spark.createDataFrame(data1, [\"tripletID\", \"triplet\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                             |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|# Apache Spark                                                                                                                                                                                                    |\n",
      "|                                                                                                                                                                                                                  |\n",
      "|Spark is a unified analytics engine for large-scale data processing. It provides                                                                                                                                  |\n",
      "|high-level APIs in Scala, Java, Python, and R, and an optimized engine that                                                                                                                                       |\n",
      "|supports general computation graphs for data analysis. It also supports a                                                                                                                                         |\n",
      "|rich set of higher-level tools including Spark SQL for SQL and DataFrames,                                                                                                                                        |\n",
      "|pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing,                                                                                                                |\n",
      "|and Structured Streaming for stream processing.                                                                                                                                                                   |\n",
      "|                                                                                                                                                                                                                  |\n",
      "|<https://spark.apache.org/>                                                                                                                                                                                       |\n",
      "|                                                                                                                                                                                                                  |\n",
      "|[![GitHub Actions Build](https://github.com/apache/spark/actions/workflows/build_main.yml/badge.svg)](https://github.com/apache/spark/actions/workflows/build_main.yml)                                           |\n",
      "|[![AppVeyor Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)                     |\n",
      "|[![PySpark Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)                                                                                       |\n",
      "|[![PyPI Downloads](https://static.pepy.tech/personalized-badge/pyspark?period=month&units=international_system&left_color=black&right_color=orange&left_text=PyPI%20downloads)](https://pypi.org/project/pyspark/)|\n",
      "|                                                                                                                                                                                                                  |\n",
      "|                                                                                                                                                                                                                  |\n",
      "|## Online Documentation                                                                                                                                                                                           |\n",
      "|                                                                                                                                                                                                                  |\n",
      "|You can find the latest Spark documentation, including a programming                                                                                                                                              |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|max(length of words)|\n",
      "+--------------------+\n",
      "|16                  |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "path = \"C:\\spark\\spark-3.5.1-bin-hadoop3\\README.md\"\n",
    "text = spark.read.text(path)\n",
    "text.show(truncate=False)\n",
    "text.select(sf.size(sf.split(text.value, \"\\s+\")).name(\"length of words\")).agg(sf.max(sf.col('length of words'))).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|col        |\n",
      "+-----------+\n",
      "|#          |\n",
      "|Apache     |\n",
      "|Spark      |\n",
      "|           |\n",
      "|Spark      |\n",
      "|is         |\n",
      "|a          |\n",
      "|unified    |\n",
      "|analytics  |\n",
      "|engine     |\n",
      "|for        |\n",
      "|large-scale|\n",
      "|data       |\n",
      "|processing.|\n",
      "|It         |\n",
      "|provides   |\n",
      "|high-level |\n",
      "|APIs       |\n",
      "|in         |\n",
      "|Scala,     |\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text.select(sf.explode(sf.split(text.value, '\\s+'))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'aa'), (3, 'aaa')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(range(1,4)).map(lambda s:(s, 'a'*s))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsTextFile(\"C:/Users/vrjav/Downloads/pyspark/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.parallelize(range(1,10))\n",
    "counter = 0\n",
    "def inc(data):\n",
    "    global counter\n",
    "    counter += data\n",
    "rdd1.foreach(inc)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName('123').setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "accum = sc.accumulator(0)\n",
    "rdd2 = sc.parallelize(range(1,10))\n",
    "def inc1(data):\n",
    "    accum.add(data)\n",
    "rdd2.foreach(inc1)\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
