{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.enabled', True)\n",
    "df = spark.createDataFrame([['a', 13, \"ucd\"],['b', 15, \"ucb\"], ['c', 19,\"uccs\"]],schema=['name', 'age', 'university'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()\n",
    "\n",
    "df = df.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "|name|age|university|\n",
      "+----+---+----------+\n",
      "|   b| 15|       ucb|\n",
      "|   c| 19|      uccs|\n",
      "|   a| 13|       ucd|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|name|avg(age)|\n",
      "+----+--------+\n",
      "|   b|    15.0|\n",
      "|   c|    19.0|\n",
      "|   a|    13.0|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('name').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|   b|\n",
      "|   c|\n",
      "|   a|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+----------+\n",
      "|name|age|university|upper_name|\n",
      "+----+---+----------+----------+\n",
      "|   b| 15|       ucb|         B|\n",
      "|   c| 19|      uccs|         C|\n",
      "|   a| 13|       ucd|         A|\n",
      "+----+---+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "df.withColumn('upper_name', upper(df.name)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text('teenagers.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|           value|\n",
      "+----------------+\n",
      "|   a,13,abnjvjdv|\n",
      "|     b,14,jvvhsf|\n",
      "|     c,1,jdvjkhv|\n",
      "|    d,15,jvhggir|\n",
      "|    e,24,hhfrwnf|\n",
      "|  f,18,iueairier|\n",
      "|g,10,iwruiwejrsf|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.value.contains('j')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"test\").setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrjav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a,13,abnjvjdv',\n",
       " 'b,14,jvvhsf',\n",
       " 'c,1,jdvjkhv',\n",
       " 'd,15,jvhggir',\n",
       " 'e,24,hhfrwnf',\n",
       " 'f,18,iueairier',\n",
       " 'g,10,iwruiwejrsf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sql_sc = SQLContext(sc)\n",
    "df1 = sc.textFile('teenagers.txt')\n",
    "df1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd = df1.map(lambda x:(x,)).toDF(['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|           value|\n",
      "+----------------+\n",
      "|   a,13,abnjvjdv|\n",
      "|     b,14,jvvhsf|\n",
      "|     c,1,jdvjkhv|\n",
      "|    d,15,jvhggir|\n",
      "|  f,18,iueairier|\n",
      "|g,10,iwruiwejrsf|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rdd.filter(df_rdd.value.contains('1')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = spark.sparkContext.parallelize(range(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|tripletID|   triplet|\n",
      "+---------+----------+\n",
      "|  0, 1, 2| [0, 1, 2]|\n",
      "|  0, 1, 3| [0, 1, 3]|\n",
      "|  0, 1, 4| [0, 1, 4]|\n",
      "|  0, 1, 5| [0, 1, 5]|\n",
      "|  0, 1, 6| [0, 1, 6]|\n",
      "|  0, 1, 7| [0, 1, 7]|\n",
      "|  0, 1, 8| [0, 1, 8]|\n",
      "|  0, 1, 9| [0, 1, 9]|\n",
      "| 0, 1, 10|[0, 1, 10]|\n",
      "| 0, 1, 11|[0, 1, 11]|\n",
      "| 0, 1, 12|[0, 1, 12]|\n",
      "| 0, 1, 13|[0, 1, 13]|\n",
      "| 0, 1, 14|[0, 1, 14]|\n",
      "| 0, 1, 15|[0, 1, 15]|\n",
      "| 0, 1, 16|[0, 1, 16]|\n",
      "| 0, 1, 17|[0, 1, 17]|\n",
      "| 0, 1, 18|[0, 1, 18]|\n",
      "| 0, 1, 19|[0, 1, 19]|\n",
      "| 0, 1, 20|[0, 1, 20]|\n",
      "| 0, 1, 21|[0, 1, 21]|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 43569)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Users\\vrjav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Users\\vrjav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"C:\\Users\\vrjav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vrjav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "combinationsDF = list(combinations(val.collect(), 3))\n",
    "\n",
    "data1 = [(f\"{sorted(row)[0]}, {sorted(row)[1]}, {sorted(row)[2]}\", list(sorted(row))) for row in combinationsDF]\n",
    "df2 = spark.createDataFrame(data1, [\"tripletID\", \"triplet\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
