{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('C:/Users/vrjav/Downloads/pyspark/learning1.ndjson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"details\")\n",
    "spark.sql('select * from details where name=\"Michael\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Load and Save Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.json\"\n",
    "\n",
    "df = spark.read.load(path, format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources\"\n",
    "\n",
    "df1 = spark.read.load(os.path.join(path, \"people.json\"), format = 'json')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          NULL|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.load(os.path.join(path, \"users.parquet\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SQL on files directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"select * from parquet.`C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/users.parquet`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          NULL|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(os.path.join(path,\"users.orc\"), format='orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orc = df.write \\\n",
    "    .format(\"orc\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"users_without_bloom.orc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bloom = df.write \\\n",
    "    .format(\"orc\") \\\n",
    "    .option(\"orc.bloom.filter.columns\", \"favorite_color\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"users_with_bloom.orc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orc = spark.read.format(\"orc\").load(\"users_without_bloom.orc\")\n",
    "df_bloom = spark.read.format(\"orc\").load(\"users_with_bloom.orc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time without Bloom filter: 1.999 seconds\n",
      "Time with Bloom filter: 0.462 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "# Time query function\n",
    "def time_query(df):\n",
    "    start = time.time()\n",
    "    df.filter(col(\"favorite_color\") == \"blue\").count()\n",
    "    return time.time() - start\n",
    "\n",
    "# Without Bloom filter\n",
    "time_no_bloom = time_query(df_orc)\n",
    "\n",
    "# With Bloom filter\n",
    "time_with_bloom = time_query(df_bloom)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Time without Bloom filter: {time_no_bloom:.3f} seconds\")\n",
    "print(f\"Time with Bloom filter: {time_with_bloom:.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic Files Source Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Source Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         file|\n",
      "+-------------+\n",
      "|file1.parquet|\n",
      "|file2.parquet|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"ignoreCorruptFiles\", \"true\").parquet(\"C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/dir1\", \"C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/dir1/dir2\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         file|\n",
      "+-------------+\n",
      "|file1.parquet|\n",
      "|file2.parquet|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"set spark.sql.files.ignoreCorruptFiles=true\")\n",
    "df0 = spark.read.parquet(\"C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/dir1\", \"C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/dir1/dir2\")\n",
    "df0.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recursive File Lookup\n",
    "In here we don't need to mention subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         file|\n",
      "+-------------+\n",
      "|file1.parquet|\n",
      "|file2.parquet|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").option(\"recursiveFileLookup\", 'true').load(\"C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/dir1\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import  SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pq = spark.read.json(\"C:/Users/vrjav/Downloads/pyspark/learning1.ndjson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pq.write.parquet('people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pqFile = spark.read.parquet('people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pqFile.createOrReplaceTempView('example')\n",
    "ex_pq = spark.sql('select * from example where age is Null')\n",
    "ex_pq.show()\n",
    "ex_pq.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "sc = spark.sparkContext\n",
    "sq_pq = spark.createDataFrame(sc.parallelize(range(1,6)).map(lambda i: Row(single=i, double=i**2)))\n",
    "sq_pq.write.parquet('data/test_table/key=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_pq = spark.createDataFrame(sc.parallelize(range(6,11)).map(lambda i: Row(single=i, cube=i**3)))\n",
    "cb_pq.write.parquet('data/test_table/key=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+---+\n",
      "|single|double|cube|key|\n",
      "+------+------+----+---+\n",
      "|     1|     1|NULL|  1|\n",
      "|     2|     4|NULL|  1|\n",
      "|     3|     9|NULL|  1|\n",
      "|     4|    16|NULL|  1|\n",
      "|     5|    25|NULL|  1|\n",
      "|     6|  NULL| 216|  2|\n",
      "|     7|  NULL| 343|  2|\n",
      "|     9|  NULL| 729|  2|\n",
      "|     8|  NULL| 512|  2|\n",
      "|    10|  NULL|1000|  2|\n",
      "+------+------+----+---+\n",
      "\n",
      "root\n",
      " |-- single: long (nullable = true)\n",
      " |-- double: long (nullable = true)\n",
      " |-- cube: long (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mergedf = spark.read.option('mergeSchema', 'true').parquet('data/test_table')\n",
    "mergedf.show()\n",
    "mergedf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "sc = spark.sparkContext\n",
    "\n",
    "orc_df = spark.createDataFrame(sc.parallelize(range(1,10)).map(lambda i: Row(single=i, power4=i**4)))\n",
    "orc_df.write.orc('data/test_table/key=3', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|single|power4|\n",
      "+------+------+\n",
      "|     4|   256|\n",
      "|     5|   625|\n",
      "|     6|  1296|\n",
      "|     7|  2401|\n",
      "|     8|  4096|\n",
      "|     9|  6561|\n",
      "|     3|    81|\n",
      "|     2|    16|\n",
      "|     1|     1|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orc = spark.read.orc('data/test_table/key=3')\n",
    "df_orc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|      name|         address|\n",
      "+----------+----------------+\n",
      "|Vidyaranya|{Columbus, Ohio}|\n",
      "+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_strings = ['{\"name\":\"Vidyaranya\", \"address\": {\"city\":\"Columbus\",\"state\":\"Ohio\"}}']\n",
    "\n",
    "\n",
    "json_df = sc.parallelize(json_strings)\n",
    "people_df = spark.read.json(json_df)\n",
    "people_df.select('name', 'address').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|       address|      name|\n",
      "+--------------+----------+\n",
      "|Columbus, Ohio|Vidyaranya|\n",
      "+--------------+----------+\n",
      "\n",
      "+----------+--------------+\n",
      "|      name|       address|\n",
      "+----------+--------------+\n",
      "|Vidyaranya|Columbus, Ohio|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat_ws\n",
    "p1_df = people_df.withColumn('address', concat_ws(\", \", col('address.city'), col('address.state')))\n",
    "\n",
    "p1_df.show()\n",
    "\n",
    "p1_df = people_df.withColumn('address', concat_ws(\", \", col('address.city'), col('address.state'))).select('name', 'address')\n",
    "p1_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|name|address|\n",
      "+----+-------+\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p1_df.createOrReplaceTempView('people')\n",
    "s1 = spark.sql('select * from people where name != \"Vidyaranya\"')\n",
    "s1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|               _c0|\n",
      "+------------------+\n",
      "|      name;age;job|\n",
      "|Jorge;30;Developer|\n",
      "|  Bob;32;Developer|\n",
      "+------------------+\n",
      "\n",
      "+-----+---+---------+\n",
      "|  _c0|_c1|      _c2|\n",
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n",
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n",
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#withoutout any delimiter or option\n",
    "csv = spark.read.csv('C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.csv')\n",
    "csv.show()\n",
    "\n",
    "#with delimiter in option\n",
    "csv1 = spark.read.option(\"delimiter\", \";\").csv('C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.csv')\n",
    "csv1.show()\n",
    "\n",
    "#with multiple option \n",
    "csv2 = spark.read.option(\"delimiter\",\";\").option('header','True').csv('C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.csv')\n",
    "csv2.show()\n",
    "\n",
    "#options instead of multiple option\n",
    "csv3 = spark.read.options(delimiter=\";\", header = True).csv('C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.csv')\n",
    "csv3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+-----+---+---------+\n",
      "| name|age|      job| name|age|      job|\n",
      "+-----+---+---------+-----+---+---------+\n",
      "|Jorge| 30|Developer|Jorge| 30|Developer|\n",
      "|Jorge| 30|Developer|  Bob| 32|Developer|\n",
      "|  Bob| 32|Developer|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|  Bob| 32|Developer|\n",
      "+-----+---+---------+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv3.createOrReplaceTempView(\"employee\")\n",
    "csv_sql = spark.sql('select * from employee e1 full join employee e2 ')\n",
    "csv_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
