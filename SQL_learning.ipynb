{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('C:/Users/vrjav/Downloads/pyspark/learning1.ndjson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"details\")\n",
    "spark.sql('select * from details where name=\"Michael\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Load and Save Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.json\"\n",
    "\n",
    "df = spark.read.load(path, format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources\"\n",
    "\n",
    "df1 = spark.read.load(os.path.join(path, \"people.json\"), format = 'json')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          NULL|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.load(os.path.join(path, \"users.parquet\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SQL on files directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"select * from parquet.`C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/users.parquet`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          NULL|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(os.path.join(path,\"users.orc\"), format='orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orc = df.write \\\n",
    "    .format(\"orc\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"users_without_bloom.orc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bloom = df.write \\\n",
    "    .format(\"orc\") \\\n",
    "    .option(\"orc.bloom.filter.columns\", \"favorite_color\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"users_with_bloom.orc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orc = spark.read.format(\"orc\").load(\"users_without_bloom.orc\")\n",
    "df_bloom = spark.read.format(\"orc\").load(\"users_with_bloom.orc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time without Bloom filter: 1.999 seconds\n",
      "Time with Bloom filter: 0.462 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "# Time query function\n",
    "def time_query(df):\n",
    "    start = time.time()\n",
    "    df.filter(col(\"favorite_color\") == \"blue\").count()\n",
    "    return time.time() - start\n",
    "\n",
    "# Without Bloom filter\n",
    "time_no_bloom = time_query(df_orc)\n",
    "\n",
    "# With Bloom filter\n",
    "time_with_bloom = time_query(df_bloom)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Time without Bloom filter: {time_no_bloom:.3f} seconds\")\n",
    "print(f\"Time with Bloom filter: {time_with_bloom:.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic Files Source Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Source Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         file|\n",
      "+-------------+\n",
      "|file1.parquet|\n",
      "|file2.parquet|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"ignoreCorruptFiles\", \"true\").parquet(\"C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/dir1\", \"C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/dir1/dir2\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         file|\n",
      "+-------------+\n",
      "|file1.parquet|\n",
      "|file2.parquet|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"set spark.sql.files.ignoreCorruptFiles=true\")\n",
    "df0 = spark.read.parquet(\"C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/dir1\", \"C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/dir1/dir2\")\n",
    "df0.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recursive File Lookup\n",
    "In here we don't need to mention subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         file|\n",
      "+-------------+\n",
      "|file1.parquet|\n",
      "|file2.parquet|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").option(\"recursiveFileLookup\", 'true').load(\"C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/dir1\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import  SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pq = spark.read.json(\"C:/Users/vrjav/Downloads/pyspark/learning1.ndjson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pq.write.parquet('people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pqFile = spark.read.parquet('people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pqFile.createOrReplaceTempView('example')\n",
    "ex_pq = spark.sql('select * from example where age is Null')\n",
    "ex_pq.show()\n",
    "ex_pq.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "sc = spark.sparkContext\n",
    "sq_pq = spark.createDataFrame(sc.parallelize(range(1,6)).map(lambda i: Row(single=i, double=i**2)))\n",
    "sq_pq.write.parquet('data/test_table/key=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_pq = spark.createDataFrame(sc.parallelize(range(6,11)).map(lambda i: Row(single=i, cube=i**3)))\n",
    "cb_pq.write.parquet('data/test_table/key=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+---+\n",
      "|single|double|cube|key|\n",
      "+------+------+----+---+\n",
      "|     1|     1|NULL|  1|\n",
      "|     2|     4|NULL|  1|\n",
      "|     3|     9|NULL|  1|\n",
      "|     4|    16|NULL|  1|\n",
      "|     5|    25|NULL|  1|\n",
      "|     6|  NULL| 216|  2|\n",
      "|     7|  NULL| 343|  2|\n",
      "|     9|  NULL| 729|  2|\n",
      "|     8|  NULL| 512|  2|\n",
      "|    10|  NULL|1000|  2|\n",
      "+------+------+----+---+\n",
      "\n",
      "root\n",
      " |-- single: long (nullable = true)\n",
      " |-- double: long (nullable = true)\n",
      " |-- cube: long (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mergedf = spark.read.option('mergeSchema', 'true').parquet('data/test_table')\n",
    "mergedf.show()\n",
    "mergedf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "sc = spark.sparkContext\n",
    "\n",
    "orc_df = spark.createDataFrame(sc.parallelize(range(1,10)).map(lambda i: Row(single=i, power4=i**4)))\n",
    "orc_df.write.orc('data/test_table/key=3', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|single|power4|\n",
      "+------+------+\n",
      "|     4|   256|\n",
      "|     5|   625|\n",
      "|     6|  1296|\n",
      "|     7|  2401|\n",
      "|     8|  4096|\n",
      "|     9|  6561|\n",
      "|     3|    81|\n",
      "|     2|    16|\n",
      "|     1|     1|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orc = spark.read.orc('data/test_table/key=3')\n",
    "df_orc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|      name|         address|\n",
      "+----------+----------------+\n",
      "|Vidyaranya|{Columbus, Ohio}|\n",
      "+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_strings = ['{\"name\":\"Vidyaranya\", \"address\": {\"city\":\"Columbus\",\"state\":\"Ohio\"}}']\n",
    "\n",
    "\n",
    "json_df = sc.parallelize(json_strings)\n",
    "people_df = spark.read.json(json_df)\n",
    "people_df.select('name', 'address').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|       address|      name|\n",
      "+--------------+----------+\n",
      "|Columbus, Ohio|Vidyaranya|\n",
      "+--------------+----------+\n",
      "\n",
      "+----------+--------------+\n",
      "|      name|       address|\n",
      "+----------+--------------+\n",
      "|Vidyaranya|Columbus, Ohio|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat_ws\n",
    "p1_df = people_df.withColumn('address', concat_ws(\", \", col('address.city'), col('address.state')))\n",
    "\n",
    "p1_df.show()\n",
    "\n",
    "p1_df = people_df.withColumn('address', concat_ws(\", \", col('address.city'), col('address.state'))).select('name', 'address')\n",
    "p1_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|name|address|\n",
      "+----+-------+\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p1_df.createOrReplaceTempView('people')\n",
    "s1 = spark.sql('select * from people where name != \"Vidyaranya\"')\n",
    "s1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|               _c0|\n",
      "+------------------+\n",
      "|      name;age;job|\n",
      "|Jorge;30;Developer|\n",
      "|  Bob;32;Developer|\n",
      "+------------------+\n",
      "\n",
      "+-----+---+---------+\n",
      "|  _c0|_c1|      _c2|\n",
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n",
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n",
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#withoutout any delimiter or option\n",
    "csv = spark.read.csv('C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.csv')\n",
    "csv.show()\n",
    "\n",
    "#with delimiter in option\n",
    "csv1 = spark.read.option(\"delimiter\", \";\").csv('C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.csv')\n",
    "csv1.show()\n",
    "\n",
    "#with multiple option \n",
    "csv2 = spark.read.option(\"delimiter\",\";\").option('header','True').csv('C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.csv')\n",
    "csv2.show()\n",
    "\n",
    "#options instead of multiple option\n",
    "csv3 = spark.read.options(delimiter=\";\", header = True).csv('C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.csv')\n",
    "csv3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+-----+---+---------+\n",
      "| name|age|      job| name|age|      job|\n",
      "+-----+---+---------+-----+---+---------+\n",
      "|Jorge| 30|Developer|Jorge| 30|Developer|\n",
      "|Jorge| 30|Developer|  Bob| 32|Developer|\n",
      "|  Bob| 32|Developer|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|  Bob| 32|Developer|\n",
      "+-----+---+---------+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv3.createOrReplaceTempView(\"employee\")\n",
    "csv_sql = spark.sql('select * from employee e1 full join employee e2 ')\n",
    "csv_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|      value|\n",
      "+-----------+\n",
      "|Michael, 29|\n",
      "|   Andy, 30|\n",
      "| Justin, 19|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|      value|\n",
      "+-----------+\n",
      "|    Michael|\n",
      "|   29\\nAndy|\n",
      "| 30\\nJustin|\n",
      "|       19\\n|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|      value|\n",
      "+-----------+\n",
      "|    Michael|\n",
      "|   29\\nAndy|\n",
      "| 30\\nJustin|\n",
      "|       19\\n|\n",
      "+-----------+\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|Michael, 29\\nAndy...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf = spark.read.text('C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.txt')\n",
    "tf.show()\n",
    "\n",
    "tf1 = spark.read.option(\"linesep\",',').text('C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.txt')\n",
    "tf1.show()\n",
    "\n",
    "tf2 = spark.read.text('C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.txt', lineSep=',')\n",
    "tf2.show()\n",
    "\n",
    "tf3 = spark.read.text('C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.txt',  wholetext=True)\n",
    "tf3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|_c0|    _c1|\n",
      "+---+-------+\n",
      "|238|val_238|\n",
      "| 86| val_86|\n",
      "|311|val_311|\n",
      "+---+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---+---+\n",
      "|_c0|_c1|\n",
      "+---+---+\n",
      "|238|238|\n",
      "| 86| 86|\n",
      "|311|311|\n",
      "+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Key='238', val='val_238'),\n",
       " Row(Key='86', val='val_86'),\n",
       " Row(Key='311', val='val_311'),\n",
       " Row(Key='27', val='val_27'),\n",
       " Row(Key='165', val='val_165')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "from pyspark.sql.functions import col, split, regexp_replace\n",
    "kv1 = spark.read.options(delimiter=\"\u0001\").csv(\"C:/spark/spark-3.5.1-bin-hadoop3/examples/src/main/resources/kv1.txt\")\n",
    "kv1.show(3)\n",
    "\n",
    "kv1_updated = kv1.withColumn(\"_c1\",  regexp_replace(col('_c1'), 'val_', \"\"))\n",
    "kv1_updated.show(3) \n",
    "\n",
    "\n",
    "\n",
    "kv1_r1 = kv1.withColumnRenamed('_c0', 'Key').withColumnRenamed('_c1', 'val')\n",
    "kv1_r1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|key|   val|\n",
      "+---+------+\n",
      "|  0| val_0|\n",
      "|  0| val_0|\n",
      "|  0| val_0|\n",
      "|  2| val_2|\n",
      "|  4| val_4|\n",
      "|  5| val_5|\n",
      "|  5| val_5|\n",
      "|  5| val_5|\n",
      "|  8| val_8|\n",
      "|  9| val_9|\n",
      "| 10|val_10|\n",
      "| 11|val_11|\n",
      "| 12|val_12|\n",
      "| 12|val_12|\n",
      "| 15|val_15|\n",
      "| 15|val_15|\n",
      "| 17|val_17|\n",
      "| 18|val_18|\n",
      "| 18|val_18|\n",
      "| 19|val_19|\n",
      "+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kv1_r1 = kv1_r1.withColumn('key', kv1_r1['key'].cast('int'))\n",
    "kv1_r1.createOrReplaceTempView(\"values\")\n",
    "spark.sql(\"select * from values where key<100 order by cast(key as INT) \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: integer (nullable = true)\n",
      " |-- val: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kv1_r1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "|key|    val|value_|\n",
      "+---+-------+------+\n",
      "|238|val_238|   238|\n",
      "| 86| val_86|    86|\n",
      "|311|val_311|   311|\n",
      "| 27| val_27|    27|\n",
      "|165|val_165|   165|\n",
      "|409|val_409|   409|\n",
      "|255|val_255|   255|\n",
      "|278|val_278|   278|\n",
      "| 98| val_98|    98|\n",
      "|484|val_484|   484|\n",
      "+---+-------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+\n",
      "|total number of rows|\n",
      "+--------------------+\n",
      "|                 500|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col, regexp_replace\n",
    "kv1_u = kv1_r1.withColumn('value_', regexp_replace(col('val'), \"val_\", \"\")).select(\"key\", \"val\", \"value_\")\n",
    "\n",
    "kv1_u.createOrReplaceTempView('example')\n",
    "kv1_sql = spark.sql('select * from example where key = value_')\n",
    "kv1_sql.show(10)\n",
    "\n",
    "kv1_sql1 = spark.sql(\"select count(*) as `total number of rows` from example\")\n",
    "kv1_sql1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|key| value|\n",
      "+---+------+\n",
      "|  1| val_1|\n",
      "|  2| val_2|\n",
      "|  3| val_3|\n",
      "|  4| val_4|\n",
      "|  5| val_5|\n",
      "|  6| val_6|\n",
      "|  7| val_7|\n",
      "|  8| val_8|\n",
      "|  9| val_9|\n",
      "| 10|val_10|\n",
      "| 11|val_11|\n",
      "| 12|val_12|\n",
      "| 13|val_13|\n",
      "| 14|val_14|\n",
      "| 15|val_15|\n",
      "| 16|val_16|\n",
      "| 17|val_17|\n",
      "| 18|val_18|\n",
      "| 19|val_19|\n",
      "| 20|val_20|\n",
      "+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "Record = Row('key', 'value')\n",
    "\n",
    "df = spark.createDataFrame([Record(i, 'val_'+str(i)) for i in range(1,101)])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
